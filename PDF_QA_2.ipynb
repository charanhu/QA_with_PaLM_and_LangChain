{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import getpass\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import GooglePalmEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import GooglePalm\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty list to store the documents\n",
    "documents = []\n",
    "\n",
    "# loop through all files in the \"docs\" directory\n",
    "for file in os.listdir(\"docs\"):\n",
    "    # if the file is a PDF, load it using the PyPDFLoader and add the resulting documents to the list\n",
    "    if file.endswith(\".pdf\"):\n",
    "        pdf_path = \"./docs/\" + file\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        documents.extend(loader.load())\n",
    "    # if the file is a Word document, load it using the Docx2txtLoader and add the resulting documents to the list\n",
    "    elif file.endswith('.docx') or file.endswith('.doc'):\n",
    "        doc_path = \"./docs/\" + file\n",
    "        loader = Docx2txtLoader(doc_path)\n",
    "        documents.extend(loader.load())\n",
    "    # if the file is a plain text file, load it using the TextLoader and add the resulting documents to the list\n",
    "    elif file.endswith('.txt'):\n",
    "        text_path = \"./docs/\" + file\n",
    "        loader = TextLoader(text_path)\n",
    "        documents.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a text splitter object with a chunk size of 1000 and overlap of 10\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=10)\n",
    "\n",
    "# split the documents into chunks using the text splitter\n",
    "documents = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vector database using the Chroma vector store, with documents as input\n",
    "# use the GooglePalmEmbeddings for embedding, and persist the database to the \"./data\" directory\n",
    "vectordb = Chroma.from_documents(documents, embedding=GooglePalmEmbeddings(google_api_key=api_key), persist_directory=\"./data\")\n",
    "\n",
    "# persist the vector database\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a ConversationalRetrievalChain object for PDF question answering\n",
    "pdf_qa = ConversationalRetrievalChain.from_llm(\n",
    "    GooglePalm(google_api_key=api_key),  # use GooglePalm for language modeling\n",
    "    vectordb.as_retriever(search_kwargs={'k': 6}),  # use the Chroma vector store for document retrieval\n",
    "    return_source_documents=True,  # return the source documents along with the answers\n",
    "    verbose=False  # do not print verbose output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Welcome to the DocBot. You are now ready to start interacting with your documents\n",
      "---------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# create an empty list to store the chat history\n",
    "chat_history = []\n",
    "\n",
    "# print a welcome message to the user\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "print('Welcome to the DocBot. You are now ready to start interacting with your documents')\n",
    "print('---------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Large Language Models as Optimizers (OPRO) is a framework that uses large language models (LLMs) to optimize for different objectives. OPRO can be used for tasks such as prompt optimization, where the goal is to find a prompt that optimizes the task accuracy, and code generation, where the goal is to generate code that solves a given problem.\n",
      "Answer: Varun Nair, Elliot Schumacher, Geoffrey Tso, and Anitha Kannan\n",
      "Answer: GSM8K GSM8K (Grade School Math 8K) is a dataset of 7,473 grade-school math word problems\n",
      "from the Math 8 curriculum in the United States. The dataset is split into a training set of 6,223\n",
      "examples and a test set of 1,250 examples.\n",
      "\n",
      "Exiting\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# create an interactive loop to prompt the user for questions\n",
    "while True:\n",
    "    # prompt the user for a question\n",
    "    query = input(\"Prompt: \")\n",
    "    \n",
    "    # check if the user wants to exit the loop\n",
    "    if query == \"exit\" or query == \"quit\" or query == \"q\" or query == \"f\":\n",
    "        print('Exiting')\n",
    "        sys.exit()\n",
    "    \n",
    "    # check if the user entered an empty query\n",
    "    if query == '':\n",
    "        continue\n",
    "    \n",
    "    # use the ConversationalRetrievalChain to find the answer to the user's question\n",
    "    result = pdf_qa({\"question\": query, \"chat_history\": chat_history})\n",
    "    \n",
    "    # print the answer to the user\n",
    "    print(\"Answer: \" + result[\"answer\"])\n",
    "    \n",
    "    # add the user's question and the resulting answer to the chat history\n",
    "    chat_history.append((query, result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
