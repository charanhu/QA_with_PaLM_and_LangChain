{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pypdf\n",
    "# !pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import GooglePalmEmbeddings\n",
    "from langchain.llms import GooglePalm\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "import os\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"test.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='LARGE LANGUAGE MODELS AS OPTIMIZERS\\nChengrun Yang*Xuezhi Wang Yifeng Lu Hanxiao Liu\\nQuoc V . Le Denny Zhou Xinyun Chen*\\n{chengrun, xuezhiw, yifenglu}@google.com, 6.hanxiao@gmail.com\\n{qvl, dennyzhou, xinyunchen}@google.com\\nGoogle DeepMind*Equal contribution\\nABSTRACT\\nOptimization is ubiquitous. While derivative-based algorithms have been powerful\\ntools for various problems, the absence of gradient imposes challenges on many\\nreal-world applications. In this work, we propose Optimization by PROmpting\\n(OPRO), a simple and effective approach to leverage large language models (LLMs)\\nas optimizers, where the optimization task is described in natural language. In\\neach optimization step, the LLM generates new solutions from the prompt that\\ncontains previously generated solutions with their values, then the new solutions\\nare evaluated and added to the prompt for the next optimization step. We first\\nshowcase OPRO on linear regression and traveling salesman problems, then move\\non to prompt optimization where the goal is to find instructions that maximize\\nthe task accuracy. With a variety of LLMs, we demonstrate that the best prompts\\noptimized by OPRO outperform human-designed prompts by up to 8%on GSM8K,\\nand by up to 50% on Big-Bench Hard tasks.\\n050100150# steps50.060.070.080.0training accuracy\\nGSM8K\\n(a) GSM8K\\n050100150200# steps60.080.0100.0training accuracy\\nBBHmovie_recommendation (b) BBH movie_recommendation\\nFigure 1: Prompt optimization on GSM8K (Cobbe et al., 2021) and BBH (Suzgun et al., 2022)\\nmovie_recommendation. The optimization on GSM8K has pre-trained PaLM 2-L as the scorer and\\nthe instruction-tuned PaLM 2-L (denoted PaLM 2-L-IT ) as the optimizer; the optimization on\\nBBH movie_recommendation has text-bison as the scorer and PaLM 2-L-IT as the optimizer.\\nSee Section 5 for more details on experimental setup.\\nTable 1: Top instructions with the highest GSM8K zero-shot test accuracies from prompt optimization\\nwith different optimizer LLMs. All results use the pre-trained PaLM 2-L as the scorer. Source Instruction Acc\\nBaselines\\n(Kojima et al., 2022) Let’s think step by step. 71.8\\n(Zhou et al., 2022b) Let’s work this out in a step by step way to be sure we have the right answer. 58.8\\n(empty string) 34.0\\nOurs\\nPaLM 2-L-IT Take a deep breath and work on this problem step-by-step. 80.2\\nPaLM 2-L Break this down. 79.9\\ngpt-3.5-turbo A little bit of arithmetic and a logical approach will help us quickly arrive at\\nthe solution to this problem.78.5\\ngpt-4 Let’s combine our numerical command and clear thinking to quickly and\\naccurately decipher the answer.74.5\\n1arXiv:2309.03409v1  [cs.LG]  7 Sep 2023', metadata={'source': 'test.pdf', 'page': 0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PALM_API_KEY'] = getpass.getpass('PALM_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15: Large Language Models as Optimizers\n",
      "050100150200# steps50.060.070.0accuracyascending (default)descendingrandom\n",
      "(a) instruction ordering (GSM8K)\n",
      "050100150200# steps0.050.0100.0accuracyascending (default)descendingrandom (b) instruction ordering (BBH sports_understanding)\n",
      "050100150200# steps50.060.070\n",
      "8: Large Language Models as Optimizers\n",
      "To examine the transferability of the optimized instructions, we also evaluate the instructions op-\n",
      "timized for GSM8K on two other mathematical reasoning datasets, i.e., MultiArith (Roy & Roth,\n",
      "2016) and AQuA (Ling et al., 2017).\n",
      "Implementation details. We set the\n",
      "10: Large Language Models as Optimizers\n",
      "050100150200# steps50.060.070.0training accuracy\n",
      "GSM8K(scorer: text-bison)\n",
      "(a)PaLM 2-L-IT optimizer\n",
      "020406080# steps20.040.060.080.0training accuracy\n",
      "GSM8K(scorer and optimizer:PaLM 2-L) (b) pre-trained PaLM 2-L optimizer\n",
      "Figure 4: Prompt optimization on GSM8K wit\n",
      "16: Large Language Models as Optimizers\n",
      "040080012001600# evaluated instructions50.060.070.0accuracy1248 (default)16\n",
      "(a) GSM8K\n",
      "040080012001600# evaluated instructions0.050.0100.0accuracy1248 (default)16 (b) BBH sports_understanding\n",
      "Figure 8: Ablation studies: the number of generated instructions in each \n",
      "14: Large Language Models as Optimizers\n",
      "Table 6: Transferability across datasets: accuracies of top instructions found for GSM8K on Multi-\n",
      "Arith and AQuA.\n",
      "Scorer SourceInstruction\n",
      "positionInstructionAccuracy\n",
      "MultiArith AQuA\n",
      "Baselines\n",
      "PaLM 2-L (Kojima et al.,\n",
      "2022)A_begin Let’s think step by step. 85.7 4\n"
     ]
    }
   ],
   "source": [
    "faiss_index = FAISS.from_documents(pages, GooglePalmEmbeddings(google_api_key=os.environ['PALM_API_KEY']))\n",
    "\n",
    "docs = faiss_index.similarity_search(\"Ablation studies: the number of generated instructions in each step\", k=5)\n",
    "\n",
    "for doc in docs:\n",
    "    print(str(doc.metadata[\"page\"]) + \":\", doc.page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
